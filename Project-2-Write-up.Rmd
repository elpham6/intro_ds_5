---
title: "DATS6101 Project 2 - Examination of Risk Factors Strongly Associated with the Likelihood of Developing Diabetes in the United States"
author: Group 5 - Erika Pham, Mohamed Sillah Kanu, Sri Varshini Yaddanapudi
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
date: "2023-05-02"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F, message = F)
options(scientific = T, digits = 3)
```

```{r init, include = FALSE}
# loading in libraries
library(ezids)
library(tidyverse)
library(readr)
library(ggplot2)
library(stringr)
library(gridExtra)
library(MASS)
library(rpart)
library(caret)
library(pROC)
library(leaps)
library(ISLR)
library(ggthemes)
library(data.table)
library(ggthemr)
library(ROCR)
library(ggpubr)
library(grid)
```


```{r, include = FALSE}
# loading in dataset
diabetes50 <- read.csv("diabetes_binary_5050split_health_indicators_BRFSS2015.csv")
str(diabetes50)

# renaming some longer variables
diabetes50 <- diabetes50 %>%
    rename("diabetes"= Diabetes_binary)
diabetes50 <- diabetes50 %>%
  rename("heart_dis" = HeartDiseaseorAttack)
diabetes50 <- diabetes50 %>%
  rename("hvy_alc" = HvyAlcoholConsump)

# storing categorical variables as factor
diabetes50$diabetes <- as.factor(diabetes50$diabetes)
diabetes50$HighBP <- as.factor(diabetes50$HighBP)
diabetes50$HighChol <- as.factor(diabetes50$HighChol)
diabetes50$CholCheck <- as.factor(diabetes50$CholCheck)
diabetes50$Smoker <- as.factor(diabetes50$Smoker)
diabetes50$Stroke <- as.factor(diabetes50$Stroke)
diabetes50$NoDocbcCost <- as.factor(diabetes50$NoDocbcCost)
diabetes50$heart_dis <- as.factor(diabetes50$heart_dis)
diabetes50$PhysActivity <- as.factor(diabetes50$PhysActivity)
diabetes50$Fruits <- as.factor(diabetes50$Fruits)
diabetes50$Veggies <- as.factor(diabetes50$Veggies)
diabetes50$hvy_alc <- as.factor(diabetes50$hvy_alc)
diabetes50$DiffWalk <- as.factor(diabetes50$DiffWalk)
diabetes50$Sex <- as.factor(diabetes50$Sex)
diabetes50$GenHlth <- as.factor(diabetes50$GenHlth)
diabetes50$AnyHealthcare <- as.factor(diabetes50$AnyHealthcare)
diabetes50$Education <- as.factor(diabetes50$Education)
diabetes50$Income <- as.factor(diabetes50$Income)
diabetes50$Age <- as.factor(diabetes50$Age)


```

**Summary:**

Diabetes is one of the most common chronic diseases in the United States. According to the Centers for Disease Control and Prevention (CDC), 34.2 million Americans have diabetes and 88 million have pre-diabetes (2018).For Project 1, our group looked at a data set from [Kaggle](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset?select=diabetes_012_health_indicators_BRFSS2015.csv) containing pre-diabetes & diabetes-related health-risks. We want to assess which risk factors were most associated with diabetes. Through exploratory data analysis (EDA), we had more insights into the data and some possibilities we could explore when modelling for project 2. 

### 1. Background

*What is diabetes?* It is a common chronic condition that causes low insulin level in humans– that’s the hormone that helps us break down glucose(sugar) into energy. This cause diabetics to have high level of sugar in their blood. Diabetic patients develop pre-diabetes first, which is where you have high blood sugar level but not enough to be considered having diabetes. Many complications can result from diabetes, including heart disease, vision loss, kidney disease, limb amputation, etc.(5) 

This disease affects millions of people. In 2018, the Center for Disease Control and Prevention (CDC) reported 34.2 million Americans have diabetes and 88 million have pre-diabetes(1). It is also the most expensive chronic condition in the United States, with $1 out of every $4 in health care costs spent on treating people with diabetes(2). The estimated total financial cost of diabetes is $327 billion dollars in 2017(3).

Although there is no known cure for diabetes, many individuals can lessen its negative effects by adopting lifestyle changes such as decreasing weight, eating a healthy diet, exercising, and receiving medical care (pills, insulin injections, etc.). Predictive models for diabetes risk are significant tools for the general population and public health officials since early diagnosis can result in lifestyle changes and more successful treatment.(2)(5)

### 2. SMART question & objective

**SMART question:** what risk factors that are strongly associated with the likelihood of developing diabetes in the United States?

For the purposes of project 1, we will be exploring the data set using exploratory data analysis tools including running descriptive statistics, creating graphs and plots, and running chi-square tests. We hope to investigate and understand the data set to inform our analysis process in project 2. 

The expected outcome of this project is a better understanding of the relationship between various health indicators and diabetes. This exploration, along with further research from existing literature, could be very useful in informing future analysis, not just within the scope of this course but in general. 

### 3. Data

**What do we know about this dataset?**

Every year since 1984, the CDC conducts the Behavioral Risk Factor Surveillance System (BRFSS) - a health-related telephone survey collecting data on various health-related behaviors and risk factors. The CDC survey collects over 400,000 responses, with more than 300 questions; making it one of the largest conducted health surveys in the world.(6)

For the 2015 BRFSS survey, a total of 491,773 respondents were interviewed from all 50 states, the District of Columbia, and three US territories. The survey included questions on various health behaviors, including tobacco use, physical activity, nutrition, and chronic health conditions such as diabetes. The survey data was collected using random digit dialing (RDD) techniques, where telephone numbers were randomly generated and called to identify potential survey respondents. Respondents were then asked to answer a series of questions on their health behaviors and health status. 

The data set we used called "Diabetes Health Indicators" was subsetted and cleaned from this survey, done in 2015, and posted on Kaggle, [linked here](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset?select=diabetes_012_health_indicators_BRFSS2015.csv). Specifically, we used the file 'diabetes _ binary _ 5050split _ health _ indicators _ BRFSS2015.csv, which has 70,692 responses and 21 variables.It has an equal 50-50 split of respondents, randomly selected, with no diabetes and with either pre-diabetes or diabetes and is therefore balanced.The target variable Diabetes_binary has 2 classes. 0 is for no diabetes, and 1 is for prediabetes or diabetes.How the author, Alex Teboul, cleaned the data can be found [here](https://www.kaggle.com/code/alexteboul/diabetes-health-indicators-dataset-notebook/notebook).

**What are the limitations of this data set?**

The usual limitations apply - The "Diabetes Health Indicators" dataset only contains data on people who have received a diabetes diagnosis, which may not be typical of the overall population. The accuracy of any analysis or inferences made from the data may be impacted by the dataset's potential missing or incomplete data.The accuracy of any analysis or conclusions taken from the data may be impacted by flaws or inconsistencies in the data, such as inaccurate or misspelled entries. 

The generalizability of the results may be impacted by biases in the dataset's selection or data collection, such as the underrepresentation of age or ethnic groups. While working with health data, ethical issues including privacy concerns or potential stigmatization of people with diabetes may arise. These issues should be properly explored and addressed. 

It is worth noting that the BRFSS is a self-reported survey, meaning that the data is based on respondents' own perceptions and reports of their health behaviors and conditions. As with any self-reported survey, there may be limitations and biases in the data that should be carefully considered when using it for analysis or modeling. It is also likely that some participants in the survey have not been diagnosed with pre-diabetes or diabetes yet, but have already developed the condition. 

For this data set in particular, since the data grouped diabetic and pre-diabetic participants together as one class, there is potential missing information we could not get compared to if we were separating the two groups. 

**Features/Variables Descriptions:**

For easier comparison, we have roughly grouped the features into *3 categories*:

**I. Bio-demographical factors:**  

1. Sex:  0 indicates female and 1 represents male 

2. Age: it has been categorized into thirteen-level age 

3. Categories. Level 1 is between ages 18-24, level 2 is 25-29,3 is 30-34, 4 is 35-39, 5 is 40-44, 6 is 45-49, 7 is 50-54, 8 is 55-59, 9 is 60 - 64, 10 is 65-69, 11 is 70-74, 12 is 75-79, 13 is 80 and older. 

4. Education: The education level scale is between 1 to 6.  
Level 1 representing never attended/only kindergarten., level 2 is studied grades 1 to 8. Level 3 is grades 9 to 11, Level 4 is studied grade 12 or GED (graduated High School), Level 5 is attended college for 1 to 3 years, and lastly level 6 is attended college for over 4 years or graduated college.  

5. Income: Income scale is from 1 to 8; Level 1 is below annual income of $10,000 or less. Level 1 being $15,000 or less, 3 is $20,000 or below, 4 is $25,000 or below, 5 is $35,000 or below, 6 is $50,000 or below, level 7 is below or equal to $75,000. Lastly, level 8 is above $75,000. 


**II. Health Indicators:**  

1. HighBP: High Blood Pressure where 0 represents no high blood pressure and 1 indicates they have blood pressure. 

2. HighChol: High Cholesterol where 0 represents no high cholesterol and 1 indicates they have high cholesterol. 

3. BMI: The survey participants’ Body Mass Index  

4. Stroke: Participants have been asked if they’ve ever had a stroke and based on their responses it has been divided into 0 and 1 representing no and yes respectively.  

5. HeartDiseaseorAttack: Participants have been asked if they’ve ever had a history of heart attack or heart diseases and based on their responses it has been divided into 0 and 1 representing no and yes respectively.  

6. GenHlth: Responding to the question asked over telephone, “would you say that in general your health is on scale 1 to 5?”, participants responses have been recorded where 1 is ‘excellent’, 2 is ‘very good’, 3 is ‘good’, 4 is ‘fair’ and 5 is ‘poor’. 

7. MentHlth: Days of poor mental health have been recorded between 1 to 30 days. 

8. PhysHlth: Physical illness or injury days in the past 30 days based on scale 1 to 30 days.   

 
**III.Behavioral/lifestyle & other factors:**

1. Smoker: Responding to the question “Have you ever smoked at least 100 cigarettes i.e., five packs in your entire life?”, participants’ responses have been recorded as 0 that represents no and 1 for yes. 

2. PhysActivity: Participants’ physical activity has been taken into consideration in the past 30 days apart from their day job. Based on their responses 0 has been recorded as no and 1 for yes. 

3. Fruits: Consumes at least one or more fruits a day, 0 is no and 1 for yes. 

4. Veggies: Consumes at least one or more vegetables a day, 0 is no and 1 for yes. 

5. HvyAlcoholConsumption: For any male consuming fourteen or more drinks per week was considered a heavy alcohol consumer and for adult females it is seven or more per week, 0 is no and 1 for yes. 

6. AnyHealthcare: In response to “Have any kind of health care coverage, including health insurance, prepaid plans such as HMO etc.”, 0 represents no and 1 for yes. 

7. NoDocbcCost: When asked if the participants if there was ever a time, they needed to visit a doctor in the past 12 moths but couldn’t because of the cost, their responses are recorded as 0 which is a no and 1 for a yes.  

8. DiffWalk: In response to “Do you have serious difficulty walking or climbing stairs?”, responses have been recorded as 0 representing no and 1 for yes. 

### 4. Exploratory Data Analysis - Project 1 Summary

Because most of the variables are categorical, we looked at the distribution for each variable. Then, we ran chi-square tests to examine the relationship between the target variable 'diabetes' and the predictors. All p-values for the tests were 0.00, indicating that all the predictors have a relationship with 'diabetes' and should be included in the modeling process. 

As a data processing step, every categorical variable were converted into factor-type. The data was also split into train/test sets, with a ratio of 80:20.

```{r, comment = ""}
library(bestglm)
library(caret)
library(pROC)
library(glmnet)

set.seed(5000)
sample1 <- sample(c(TRUE, FALSE), nrow(diabetes50), replace = TRUE, prob=c(0.8,0.2))
data_train <- diabetes50[sample1, ]
data_test <- diabetes50[!sample1, ]


```

### 5. Statistical Modelling

Since 'diabetes' is a dummy variable, we are looking at a classification problem. In this case, we implemented 5 models: logistic regression, classification tree, random forests, lasso regression and K-nearest neighbors. 

#### 5.1 Logistic Regression

For logistic regression, feature selection was used to choose a model with the best result. We used stepwise selection, with AIC as a criteria. The best model, with 16 variables, is shown here on the training data. 

```{r, comment = ""}
library(DescTools)
train_model <- glm(formula = diabetes ~ HighBP + HighChol + CholCheck + BMI + 
    Stroke + heart_dis + PhysActivity + Veggies + hvy_alc + GenHlth + 
    MentHlth + PhysHlth + DiffWalk + Sex + Age + Education + 
    Income, family = binomial(link = "logit"), data = data_train)
xkabledply(train_model)
```

We also looked at the exponentials of the coefficients for the model:

```{r, comment = ""}

exp_coef_train <- exp(coef(train_model))
xkabledply(as.table(exp_coef_train))
```

From the results on the training set, we see that coefficients on health indicators, such as 'BMI' or 'Stroke' are statistically significant. For example, having high blood pressure increased the probability of having pre/diabetes by a factor of 2.06. Similarly, having high cholesterol increased the probability of having pre/diabetes by a factor of 1.73. Interestingly, with an increase of 1 day with poor physical health ('PhysHlth'), the probability of the respondent having pre/diabetes decreases by a factor of 0.9758.


Behavioral indicators have less of an effect comparatively. Including vegetables into your diet decreases the probability of having pre/diabetes by a factor of 0.93; which is statistically significant at the 99% level. 
Participating in physical activity (outside of work) does not seem to have a statistically significant effect on the probability of the respondent having pre/diabetes. Considering existing literature, which overwhelmingly suggests physical activity as a good diabetes-prevention method as well as a diabetes-management method, this is worth re-examining in the future (7). The questionnaire did not ask specifically what physical activity the participants were doing and at what level, which is a potential pit-fall and could explain why the variable did not have as much of an effect on predicting prediabetes/diabetes.

For bio-demographical indicators, we saw that older age groups, from 4 to 13 (35 years+), increases the probability of having pre/diabetes - with group 11 having the highest factor by 7.41 compared to age group 1. Higher income levels saw a decrease in the chance of having pre/diabetes, particularly of group 4-8 compared to group 1. Education does not have a statistically significant effect on the target variable. 


```{r, include = F}
test_model <- glm(formula = diabetes ~ HighBP + HighChol + CholCheck + BMI + 
    Stroke + heart_dis + PhysActivity + Veggies + hvy_alc + GenHlth + 
    MentHlth + PhysHlth + DiffWalk + Sex + Age + Education + 
    Income, family = binomial(link = "logit"), data = data_test)
```

As an initial assessment of the model, the pseudo R-squared was calculated to be `r PseudoR2(train_model, which = "McFadden")`. Only 26.8% of the variance in the target variable can be explained by this model. After running the model on the testing set, the R-squared is `r PseudoR2(test_model, which = "McFadden")`.

The R-squared value does not look good, so we looked at other model assessment measures as well. To do that, we first calculated the prediction values on both the training and testing sets. 

```{r, comment = ""}
library(pROC)
train_pred <- predict(train_model, newdata = data_train, type = "response")
data_train$train_pred = train_pred 
test_pred <- predict(test_model, newdata = data_test, type = "response")
data_test$test_pred = test_pred
```

In order to evaluate the model further, we looked at the confusion matrix for the model on both train and test sets. 

For the training set: 

```{r, comment = ""}
cm1 <- table(data_train$diabetes, data_train$train_pred<.4)
xkabledply( cm1, title = "Confusion Matrix for Logistic Regression Model (train set)" )

```

For the model on testing set:

```{r, comment = ""}

cm2 <- table(data_test$diabetes, test_model$fitted.values<.4)
xkabledply( cm2, title = "Confusion Matrix for Logistic Regression Model (test set)" )

```

```{r, include = FALSE}
acc1 <- round(100*(cm1[1,1]+cm1[2,2])/sum(cm1), digits=1) # acccuracy for train
acc2 <- round(100*(cm2[1,1]+cm2[2,2])/sum(cm2), digits=1) # acccuracy for test
p1 <- round(100*(cm1[1,1]/(cm1[1,1]+cm1[1,2])), digits = 1) # precision for train
r1 <- round(100*(cm1[1,1]/(cm1[1,1]+cm1[2,1])), digits = 1) # recall for train
f11 <- 2*((p1*r1)/(p1+r1)) # f1 for train
p2 <- round(100*(cm2[1,1]/(cm2[1,1]+cm2[1,2])), digits = 1) # precision for test
r2 <- round(100*(cm2[1,1]/(cm2[1,1]+cm2[2,1])), digits = 1) # recall for test
f12 <- 2*((p2*r2)/(p2+r2)) # f1 for test
```

From there, we calculated the model's accuracy, precision, recall and F1-score on both train/test data. The metrics do not indicate a good model, with accuracy of only 25.3 on training and 25.8 on testing. We could choose a cutoff that is <0.4 to favor high precision, as it is more favorable in this case to predict false positives than false negatives; but the accuracy would still be low (the highest accuracy was around 35). 

```{r, comment = ""}
eva_metrics_log <- matrix(nrow = 4, ncol = 2)
rownames(eva_metrics_log) <- c("Accuracy", "Precision", "Recall", "F1-Score")
colnames(eva_metrics_log) <- c("Training", "Testing")
# Assign values for accuracy
eva_metrics_log[1, 1] <- acc1   # Training accuracy
eva_metrics_log[1, 2] <- acc2  # Testing accuracy

# Assign values for precision
eva_metrics_log[2, 1] <- p1   # Training precision
eva_metrics_log[2, 2] <- p2  # Testing precision

# Assign values for recall
eva_metrics_log[3, 1] <- r1  # Training recall
eva_metrics_log[3, 2] <- r2   # Testing recall

# Assign values for f1
eva_metrics_log[4, 1] <- f11  # Training f1
eva_metrics_log[4, 2] <- f12   # Testing f1

# Create a visualization
xkabledply(eva_metrics_log)
```

We proceed to check the ROC and its AUC as another metric to test the model.  

```{r, comment = ""}
roc_train1 <- roc(diabetes~train_pred, data=data_train)
plot(roc_train1, main = "ROC curve for log model on training data")
```
```{r, comment = ""}
roc_test1 <- roc(diabetes~test_pred, data=data_test)
plot(roc_test1, main = "ROC curve for log model on testing data")
```
The AUC on the training set is `r auc(roc_train1)`, and `r auc(roc_test1)` on the testing set; both are above 0.8 which shows that the model has good predictive power. While the ROC is indeed biased towards performance on the positive class and should be considered, the AUC values still contradict what we just calculated above, and suggests an error on our part that would need to be reviewed.

#### 5.2 Classification Tree

For the classification tree, we used the same predictors as logistic regression. Immediately, we can see that high blood pressure has the highest importance. The default maximum tree depth is 3, which indicates that we do not need many predictors - we only need 4 variables for prediction: high blood pressure, general health, age group, and BMI.  

```{r, comment = ""}
library(rpart)
library(rpart.plot)
# grow tree
tree1 <- rpart(diabetes ~ HighBP + HighChol + CholCheck + BMI + 
    Stroke + heart_dis + PhysActivity + Veggies + Fruits + hvy_alc + GenHlth + 
    MentHlth + PhysHlth + DiffWalk + Sex + Age + Education + 
    Income, method="class", data=data_train)

# plot tree
tree1_plot <- rpart.plot(tree1, uniform=TRUE, main="Classification Tree for Diabetes")
#text(tree1_plot, use.n=TRUE, all=TRUE, cex=.8) # this doesn't always work for some reason

```

```{r, include = F}

# grow tree
tree1_test <- rpart(diabetes ~ HighBP + HighChol + CholCheck + BMI + 
    Stroke + heart_dis + PhysActivity + Veggies + hvy_alc + GenHlth + 
    MentHlth + PhysHlth + DiffWalk + Sex + Age + Education + 
    Income, method="class", data=data_test)

# plot tree
tree1_plot_test <- rpart.plot(tree1_test, uniform=TRUE, main="Classification Tree for Diabetes")
#text(tree1_plot, use.n=TRUE, all=TRUE, cex=.8) # this doesn't always work for some reason

```

We can see the confusion matrix for the model on training set below. The accuracy is 72.8, which reaffirms that we do not need that many variables for an accurate prediction on the status of pre/diabetes. We are also not likely to have overfitted the model as well. 

```{r, comment = ""}
source("C:/College/DATS6101/project/cf_functions.R")
tree_pred_test <- predict(tree1_test, newdata=data_test, type = "class") # predictors for test data
tree_pred_train <- predict(tree1, newdata=data_train, type="class") # predictors for train
cm3_test <- confusionMatrix(tree_pred_test, data_test$diabetes) # cf for test
cm3_train <- confusionMatrix(tree_pred_train, data_train$diabetes) # cf for train
cm3_train_draw <- draw_confusion_matrix(cm3_train) # visualize cf for train

```
The matrix for the test data is here, where the accuracy is the same as the model on the training set. Other evaluation metrics also look good, and indicate reasonable predictive power while not overfitting. 

```{r, comment = ""}
cm3_test_draw <- draw_confusion_matrix(cm3_test) # cf visualize for test
```


#### 5.3 Random Forests
After an initial fit of a random forests model that uses the default hyperparameters in R, we found that the accuracy was 95 for the training data and 74.4 on the testing data; suggesting that the model is overfitted. To address this concern, we implemented a 5-fold cross-validation with 3 repeats, and ran the model again. 

```{r, comment = ""}
set.seed(123)

# define repeated cross validation with 5 folds and 3 repeats

repeat_cv <- trainControl(method = 'repeatedcv', number=5, repeats=3)

set.seed(123)
forest<- train(diabetes ~ HighBP + HighChol + CholCheck + BMI + 
    Stroke + heart_dis + PhysActivity + Veggies + hvy_alc + GenHlth + 
    MentHlth + PhysHlth + DiffWalk + Sex + Age + Education + 
    Income, data=data_train, method='rf',trControl=repeat_cv, metric='Accuracy')
forest$finalModel
```

All evaluation metrics are more reasonable now, with accuracy of 76.4 for the training set and 74.4 on the test set. 

```{r, comment = ""}
forest_train <- predict(object=forest, newdata=data_train[, -1])
forest_test <- predict(object=forest, newdata=data_test[, -1])
cm5_train <- confusionMatrix(forest_train, data_train$diabetes)
cm5_test <- confusionMatrix(forest_test, data_test$diabetes)
cm5_draw1 <- draw_confusion_matrix(cm5_train)

```

```{r, comment = ""}
cm5_draw2 <- draw_confusion_matrix(cm5_test)
```


#### 5.4 K-Nearest Neighbors

We experimented with K-Nearest neighbors as well. The model includes all possible predictors. The variables were normalized before the model was fitted. We chose a k = sqrt(21)/2 = appx. 3, where 21 is the number of variables in the dataset. 

```{r, comment = ""}

# load the required library
library(rpart)
# read in the CSV file
diabetes_knn <- read.csv("diabetes_binary_5050split_health_indicators_BRFSS2015.csv")
diabetes_knn <- diabetes_knn %>%
    rename("diabetes"= Diabetes_binary)
diabetes_knn <- diabetes_knn %>%
  rename("heart_dis" = HeartDiseaseorAttack)
diabetes_knn <- diabetes_knn %>%
  rename("hvy_alc" = HvyAlcoholConsump)

 ##create a random number equal 80% of total number of rows
 ran <- sample(1:nrow(diabetes_knn),0.8 * nrow(diabetes_knn))
 
 ##the normalization function is created
 nor <-function(x) { (x -min(x))/(max(x)-min(x))   }
 
 ##normalization function is created
 dia_nor <- as.data.frame(lapply(diabetes_knn[,c(2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22)], nor))
 
 ##training dataset extracted
 dia_train <- dia_nor[ran,]
 
 ##test dataset extracted
 dia_test <- dia_nor[-ran,]
##the 2nd column of training dataset because that is what we need to predict about testing dataset
 ##also convert ordered factor to normal factor
 dia_target <- as.factor(diabetes_knn[ran,1])
 
 ##the actual values of 1st column of testing dataset to compare it with values that will be predicted
 ##also convert ordered factor to normal factor
 test_target <- as.factor(diabetes_knn[-ran,1])
 
 ##run knn function
 library(class)
 pr <- knn(dia_train,dia_test,cl=dia_target,k=3)
```

With the confusion matrix and the evaluation metrics table, we can see that while the model has decent accuracy, it leaves room for improvement. We could choose another k, where the accuracy could be higher. 

```{r, comment = ""}
 ##create the confusion matrix
cm6 <- table(pr, test_target)

## visualize the table
xkabledply(cm6, title = "Confusion Matrix - KNN")
```



```{r, include = F}
acc7 <- round(100*(cm6[1,1]+cm6[2,2])/sum(cm6), digits=1)
p7 <- round(100*(cm6[1,1]/(cm6[1,1]+cm6[1,2])), digits = 1)
r7 <- round(100*(cm6[1,1]/(cm6[1,1]+cm6[2,1])), digits = 1)
f17 <- 2*((p7*r7)/(p7+r7))

```

```{r, comment = ""}
eva_metrics_knn <- matrix(nrow = 4, ncol = 1)
rownames(eva_metrics_knn) <- c("Accuracy", "Precision", "Recall", "F1-Score")
colnames(eva_metrics_knn) <- c("Score")
# Assign values for accuracy
eva_metrics_knn[1, 1] <- acc7  

# Assign values for precision
eva_metrics_knn[2, 1] <- p7   # Training precision

# Assign values for recall
eva_metrics_knn[3, 1] <- r7  # Training recall

# Assign values for f1
eva_metrics_knn[4, 1] <- f17  # Training f1

# Create a visualization
xkabledply(eva_metrics_knn, title = "Evaluation Matrix - KNN")
```


#### 5.4 Lasso Regression


```{r, comment = ""}
library(glmnet)

diabetes_lasso <- read.csv("diabetes_binary_5050split_health_indicators_BRFSS2015.csv")
diabetes_lasso <- diabetes_lasso %>%
    rename("diabetes"= Diabetes_binary)
diabetes_lasso <- diabetes_lasso %>%
  rename("heart_dis" = HeartDiseaseorAttack)
diabetes_lasso <- diabetes_lasso %>%
  rename("hvy_alc" = HvyAlcoholConsump)

set.seed(5000)
sample2 <- sample(c(TRUE, FALSE), nrow(diabetes_lasso), replace = TRUE, prob=c(0.8,0.2))
data_train1 <- diabetes_lasso[sample2, ]
data_test1 <- diabetes_lasso[!sample2, ]

library(glmnet)

#defining target and predictor variables x and y

y_train <- data_train1$diabetes

x_train <- data.matrix(data_train1[,c('HighBP','HighChol','heart_dis','PhysHlth','DiffWalk','Sex','Age','CholCheck', 'BMI','hvy_alc', 'GenHlth', 'PhysHlth', 'PhysActivity', 'Income', 'Veggies', 'Fruits', 'Stroke', 'Smoker', 'Education')])

# finding optimal lambda value
cv_model1 <- cv.glmnet(x_train, y_train, alpha = 1)

#find optimal lambda value that minimizes test MSE
best_lambda1 <- cv_model1$lambda.min
#best_lambda1

plot(cv_model1)

# finding features for best model
best_model1 <- glmnet(x_train, y_train, alpha = 1, lambda = best_lambda1)
#coef(best_model1)

new1 = matrix(c(0,1,0,1,1,1,0,1), nrow=1, ncol=19)

#use lasso regression model to predict response value
predict(best_model1, s = best_lambda1, newx = new1)

y_train_predict <- predict(best_model1, s = best_lambda1, newx = x_train, type = "response")

#find SST and SSE
sst1 <- sum((y_train - mean(y_train))^2)
sse1 <- sum((y_train_predict - y_train)^2)

#find R-Squared
rsq1 <- 1 - sse1/sst1


```

```{r, comment = ""}
#defining target and predictor variables x and y

y_test <- data_test1$diabetes

x_test <- data.matrix(data_test1[,c('HighBP','HighChol','heart_dis','PhysHlth','DiffWalk','Sex','Age','CholCheck', 'BMI','hvy_alc', 'GenHlth', 'PhysHlth', 'PhysActivity', 'Income', 'Veggies', 'Fruits', 'Stroke', 'Smoker', 'Education')])

# finding optimal lambda value
cv_model2 <- cv.glmnet(x_test, y_test, alpha = 1)

#find optimal lambda value that minimizes test MSE
best_lambda2 <- cv_model2$lambda.min
#best_lambda2

plot(cv_model2)

# finding features for best model
best_model2 <- glmnet(x_test, y_test, alpha = 1, lambda = best_lambda2)
#coef(best_model2)

new2 = matrix(c(0,1,0,1,1,1,0,1), nrow=1, ncol=19)

#use lasso regression model to predict response value
predict(best_model2, s = best_lambda2, newx = new2)

y_test_predict <- predict(best_model2, s = best_lambda2, newx = x_test, type = "response")

#find SST and SSE
sst2 <- sum((y_test - mean(y_test))^2)
sse2 <- sum((y_test_predict - y_test)^2)

#find R-Squared
rsq2 <- 1 - sse2/sst2



```

As a last minute experimentation (as well as a learning opportunity), we also used lasso regression. We tried to find a lambda that would minimize MSE (which was plotted above), then run the model. All possible predictors were included. We did not have enough time to fully evaluate the model, but the R-squared for the training data is `r rsq1` and `r rsq2` for the testing data. It shows that the model's power is not great, and should be examined again to improve its fit. 


```{r, include = F}
acc3 <- 0.728 # acc for class. tree, train
p3 <- 76.1 # precision for class. tree, train
r3 <- 66.4 # recall for class. tree, train
f13 <- 70.9 # 1 for class. tree, train
```

```{r, include = F}
acc4 <- 72.8 # acc for class. tree, test
p4 <- 77.1 # precision for class. tree, test
r4 <- 64.7 # recall for class. tree, test
f14 <- 70.4 # 1 for class. tree, test
```

```{r, include = F}
# for rf train
acc5 <- 76.4
p5 <- 78.6
r5 <- 72.6
f15 <- 75.5
```

```{r, include = F}
# for rf test
acc6 <- 74.4
p6 <- 76.5
r6 <- 70.4
f16 <- 73.3
```

#### 6. Conclusion 

```{r, comment = ""}
eva_metrics <- matrix(nrow = 4, ncol = 4)
rownames(eva_metrics) <- c("Accuracy", "Precision", "Recall", "F1-Score")
colnames(eva_metrics) <- c("Log. Regression", "Classification Tree", "Random Forests", "K-NN")
# Assign values for accuracy
eva_metrics[1, 1] <- acc2   
eva_metrics[1, 2] <- acc4 
eva_metrics[1, 3] <- acc6   
eva_metrics[1, 4] <- acc7
 

# Assign values for precision
eva_metrics[2, 1] <- p2   
eva_metrics[2, 2] <- p4 
eva_metrics[2, 3] <- p6   
eva_metrics[2, 4] <- p7


# Assign values for recall
eva_metrics[3, 1] <- r2   
eva_metrics[3, 2] <- r4 
eva_metrics[3, 3] <- r6   
eva_metrics[3, 4] <- r7


# Assign values for f1
eva_metrics[4, 1] <- f12   
eva_metrics[4, 2] <- f14 
eva_metrics[4, 3] <- f16   
eva_metrics[4, 4] <- f17


# Create a visualization
xkabledply(eva_metrics, title = "Evaluation Metrics - All Models")
```



While most predictors line up with existing literature on its relationship/predictive power on pre-diabetes and diabetes, we found that including physical activity did not have any effects on pre/diabetes. This could be due to the nature of the data, which does not include the frequency or intensity of the activity. More research on pre-established literature could be done to inform potential reasons why this is the case.

After examining 5 models, we concluded that random forests is the best model with the highest accuracy. Both random forests and classification tree performed similarly well. With classification tree, we saw how much we can predict with few predictors. Lasso and logistic regression performed poorly, with very low R-squared. Further review is needed as to why they did not perform well. We could try scaling the predictors, or do more hyperparameter tuning to figure out how to improve the models. While K-nearest neighbors performed decently well, it could be improve by experimenting with different k's. 




<br>
**References**

1. Centers for Disease Control and Prevention (2020). *National Diabetes Statistics Report 2020. Estimates of diabetes and its burden in the United States.* 

2. Centers for Disease Control and Prevention. (2022, December 21). *Health and economic benefits of diabetes interventions. Centers for Disease Control and Prevention.* Retrieved March 22, 2023, from https://www.cdc.gov/chronicdisease/programs-impact/pop/diabetes.html 

3. American Diabetes Association | ADA. (n.d.). *The cost of diabetes.* Retrieved March 22, 2023, from https://diabetes.org/about-us/statistics/cost-diabetes 

4. Teboul, A. (2021, November 8). *Diabetes health indicators dataset.* Kaggle. Retrieved March 22, 2023, from https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset?select=diabetes_012_health_indicators_BRFSS2015.csv 

5. Centers for Disease Control and Prevention. (2022, July 7). *What is diabetes?* Centers for Disease Control and Prevention. Retrieved March 22, 2023, from https://www.cdc.gov/diabetes/basics/diabetes.html 

6. Centers for Disease Control and Prevention. (2014, May 16). *CDC - about BRFSS.* Centers for Disease Control and Prevention. Retrieved March 22, 2023, from https://www.cdc.gov/brfss/about/index.htm 

7. Hamasaki H. *Daily physical activity and type 2 diabetes: A review.* World J Diabetes. 2016 Jun 25;7(12):243-51. doi: 10.4239/wjd.v7.i12.243. PMID: 27350847; PMCID: PMC4914832.
