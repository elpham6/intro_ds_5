---
title: "DATS6101 Project 2 - Examination of Risk Factors Strongly Associated with the Likelihood of Developing Diabetes in the United States"
author: Group 5 - Erika Pham, Mohamed Sillah Kanu, Sri Varshini Yaddanapudi
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
date: "2023-05-02"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F, message = F)
options(scientific = T, digits = 3)
```

```{r init, include = FALSE}
# loading in libraries
library(ezids)
library(tidyverse)
library(readr)
library(ggplot2)
library(stringr)
library(gridExtra)
library(MASS)
library(rpart)
library(caret)
library(pROC)
library(leaps)
library(ISLR)
library(ggthemes)
library(data.table)
library(ggthemr)
library(ROCR)
library(ggpubr)
library(grid)
```


```{r, include = FALSE}
# loading in dataset
diabetes50 <- read.csv("diabetes_binary_5050split_health_indicators_BRFSS2015.csv")
str(diabetes50)

# renaming some longer variables
diabetes50 <- diabetes50 %>%
    rename("diabetes"= Diabetes_binary)
diabetes50 <- diabetes50 %>%
  rename("heart_dis" = HeartDiseaseorAttack)
diabetes50 <- diabetes50 %>%
  rename("hvy_alc" = HvyAlcoholConsump)

# storing categorical variables as factor
diabetes50$diabetes <- as.factor(diabetes50$diabetes)
diabetes50$HighBP <- as.factor(diabetes50$HighBP)
diabetes50$HighChol <- as.factor(diabetes50$HighChol)
diabetes50$CholCheck <- as.factor(diabetes50$CholCheck)
diabetes50$Smoker <- as.factor(diabetes50$Smoker)
diabetes50$Stroke <- as.factor(diabetes50$Stroke)
diabetes50$NoDocbcCost <- as.factor(diabetes50$NoDocbcCost)
diabetes50$heart_dis <- as.factor(diabetes50$heart_dis)
diabetes50$PhysActivity <- as.factor(diabetes50$PhysActivity)
diabetes50$Fruits <- as.factor(diabetes50$Fruits)
diabetes50$Veggies <- as.factor(diabetes50$Veggies)
diabetes50$hvy_alc <- as.factor(diabetes50$hvy_alc)
diabetes50$DiffWalk <- as.factor(diabetes50$DiffWalk)
diabetes50$Sex <- as.factor(diabetes50$Sex)
diabetes50$GenHlth <- as.factor(diabetes50$GenHlth)
diabetes50$AnyHealthcare <- as.factor(diabetes50$AnyHealthcare)
diabetes50$Education <- as.factor(diabetes50$Education)
diabetes50$Income <- as.factor(diabetes50$Income)
diabetes50$Age <- as.factor(diabetes50$Age)

```

Below, I'm separating the model into training and testing data. The ratio is 8:2.


```{r, comment = ""}
library(bestglm)
library(caret)
library(pROC)
library(glmnet)

set.seed(5000)
sample1 <- sample(c(TRUE, FALSE), nrow(diabetes50), replace = TRUE, prob=c(0.8,0.2))
data_train <- diabetes50[sample1, ]
data_test <- diabetes50[!sample1, ]


```


I'm going to write the full logistic regression model(s).

```{r, comment = ""}
full <- glm(diabetes~., data=diabetes50, family=binomial(link="logit"))
full_train <- glm(diabetes~., data=data_train, family=binomial(link="logit"))
full_test <- glm(diabetes~., data=data_test, family=binomial(link="logit"))
```

Let's perform stepwise selection to see the best model. 

```{r, comment=""}

# stepwise model, both directions
step1 <- stepAIC(full_train, direction = "both", trace = FALSE)
summary(step1)

```

Stepwise selection chose 16 variables. Let's fit the model again on= training data and then obtaining the predictive variable for diabetes on both data sets. Then we can evaluate the model. 




```{r, comment = ""}
library(DescTools)
train_model <- glm(formula = diabetes ~ HighBP + HighChol + CholCheck + BMI + 
    Stroke + heart_dis + Veggies + hvy_alc + GenHlth + DiffWalk + 
    Sex + Age + Education + Income, family = binomial(link = "logit"), 
    data = data_train)
summary(train_model)
exp(coef(train_model))
PseudoR2(train_model, which = "McFadden")
```



```{r}
test_model <- glm(formula = diabetes ~ HighBP + HighChol + CholCheck + BMI + 
    Stroke + heart_dis + PhysActivity + Veggies + hvy_alc + GenHlth + 
    MentHlth + PhysHlth + DiffWalk + Sex + Age + Education + 
    Income, family = binomial(link = "logit"), data = data_test)
summary(test_model)
PseudoR2(test_model, which = "McFadden")
```

Let's obtain the predicted value that the participant have diabetes on both training and testing set.
```{r, comment = ""}
library(pROC)
train_pred <- predict(train_model, newdata = data_train, type = "response")
data_train$train_pred = train_pred # this is adding a variable for the predicted value for diabetes
# maybe I can use this to use the ConfusionMatrixInfo etc in the example
test_pred <- predict(train_model, newdata = data_test, type = "response")
data_test$test_pred = test_pred

train_score <- ggplot( data_train, aes( train_pred, color = as.factor(diabetes) ) ) + 
  geom_density( size = 1 ) +
  ggtitle( "Training Set's Predicted Score" ) + 
  scale_color_economist( name = "data", labels = c( "negative", "positive" ) ) + 
  theme_economist()
train_score
```
```{r}
test_score <- ggplot( data_test, aes( test_pred, color = as.factor(diabetes) ) ) + 
  geom_density( size = 1 ) +
  ggtitle( "Testing Set's Predicted Score" ) + 
  scale_color_economist( name = "data", labels = c( "negative", "positive" ) ) + 
  theme_economist()
test_score

```


I want to look at an accuracy/confusion matrix. 

```{r, comment = ""}
cm1 <- table(data_train$diabetes, data_train$train_pred<.4)
xkabledply( cm1, title = "Confusion Matrix for Logistic Regression Model (reduced)" )
acc1_05 <- round(100*(cm1[1,1]+cm1[2,2])/sum(cm1), digits=1)
acc1_05

```

Accuracy is 25.2. Not good. What if I choose a different cutoff? 



Let's look at confusion  matrix for the model on the test data.
```{r, comment = ""}

cm2 <- table(data_test$diabetes, test_model$fitted.values<.4)
xkabledply( cm2, title = "Confusion Matrix for Logistic Regression Model (reduced)" )
acc2_05 <- round(100*(cm2[1,1]+cm2[2,2])/sum(cm2), digits=1)
acc2_05
```


Accuracy on test model is 25, which means it is performing similarly poorly (lol). At least it's not overfitting.
```{r}
# visualize .5 cutoff for training data
source("C:/College/DATS6101/project/unbalanced_functions.R")
cm_info <- ConfusionMatrixInfo( data = data_train, predict = "train_pred", 
                                actual = "diabetes", cutoff = .5 )
ggthemr("flat")
cm_info$plot
```
```{r}
# visualize .44 cutoff for training data
source("C:/College/DATS6101/project/unbalanced_functions.R")
cm_info <- ConfusionMatrixInfo( data = data_train, predict = "train_pred", 
                                actual = "diabetes", cutoff = .4 )
ggthemr("flat")
cm_info$plot
```


```{r}
# visualize .5 cutoff 
cm_info <- ConfusionMatrixInfo( data = data_test, predict = "test_pred", 
                                actual = "diabetes", cutoff = .5 )
ggthemr("flat")
cm_info$plot
```

```{r}
# visualize .4 cutoff 
cm_info <- ConfusionMatrixInfo( data = data_test, predict = "test_pred", 
                                actual = "diabetes", cutoff = .44 )
ggthemr("flat")
cm_info$plot
```
Let's see if finding the ROC can help with some evaluation and tuning. 

```{r, comment = ""}

roc_train1 <- roc(diabetes~train_pred, data=data_train)
plot(roc_train1)
auc(roc_train1)
```

```{r, comment = ""}

roc_test1 <- roc(diabetes~test_pred, data=data_test)
plot(roc_test1)
auc(roc_test1)
```

AUC is 0.825. This shouldn't happen because the data set is balanced. I have no idea why this is happening. 


I want to look at other metrics to see where to go next: precision, recall, f1 score

Precision : True Positive / (True Positive + False Positive)

Recall = TruePositives / (TruePositives + FalseNegatives)

F1 Score = 2 * (Precision * Recall) / (Precision + Recall)

```{r, comment = ""}
p1 <- round(100*(cm1[1,1]/(cm1[1,1]+cm1[1,2])), digits = 1)
p1
r1 <- round(100*(cm1[1,1]/(cm1[1,1]+cm1[2,1])), digits = 1)
r1
f11 <- 2*((p1*r1)/(p1+r1))
f11
```

```{r, comment = ""}
p2 <- round(100*(cm2[1,1]/(cm2[1,1]+cm2[1,2])), digits = 1)
p2
r2 <- round(100*(cm2[1,1]/(cm2[1,1]+cm2[2,1])), digits = 1)
r2
f12 <- 2*((p2*r2)/(p2+r2))
f12
```

```{r}
ROCR_pred_test <- prediction(train_pred, data_train$diabetes)
ROCR_perf_test <- performance(ROCR_pred_test,'tpr','fpr')
plot(ROCR_perf_test,colorize=TRUE,print.cutoffs.at=seq(0.1,by=0.1))
```

```{r}
cost_perf = performance(ROCR_pred_test, "cost") 
ROCR_pred_test@cutoffs[[1]][which.min(cost_perf@y.values[[1]])]
```

Tree time:

```{r}
library(rpart)
library(rpart.plot)
# grow tree
tree1 <- rpart(diabetes ~ HighBP + HighChol + CholCheck + BMI + 
    Stroke + heart_dis + PhysActivity + Veggies + hvy_alc + GenHlth + 
    MentHlth + PhysHlth + DiffWalk + Sex + Age + Education + 
    Income, method="class", data=data_train)
# display results
printcp(tree1)
# visualize cv results
plotcp(tree1)
# summary of splits
summary(tree1)

# plot tree
tree1_plot <- rpart.plot(tree1, uniform=TRUE, main="Classification Tree for Diabetes")
#text(tree1_plot, use.n=TRUE, all=TRUE, cex=.8) # this doesn't always work for some reason

# create pretty plot of tree
post(tree1, file = "C:/College/DATS6101/project/intro_ds_5/tree.ps",
     title = "Classification Tree for Diabetes")
```

```{r}
# prune the tree
tree2 <- prune(tree1, cp=tree1$cptable[which.min(tree1$cptable[,"xerror"]),"CP"])

# plot the tree
rpart.plot(tree2, uniform=T, main="Pruned Classification Tree for Diabetes")
#text(tree2, use.n=T, all=T, cex=.8)
post(tree2, file="C:/College/DATS6101/project/intro_ds_5/pruned_tree.ps", 
     title="Pruned Classification Tree for Diabetes")


```


```{r, comment = ""}

# grow tree
tree1_test <- rpart(diabetes ~ HighBP + HighChol + CholCheck + BMI + 
    Stroke + heart_dis + PhysActivity + Veggies + hvy_alc + GenHlth + 
    MentHlth + PhysHlth + DiffWalk + Sex + Age + Education + 
    Income, method="class", data=data_test)
# display results
printcp(tree1_test)
# visualize cv results
plotcp(tree1_test)
# summary of splits
summary(tree1_test)

# plot tree
tree1_plot_test <- rpart.plot(tree1_test, uniform=TRUE, main="Classification Tree for Diabetes")
#text(tree1_plot, use.n=TRUE, all=TRUE, cex=.8) # this doesn't always work for some reason

# create pretty plot of tree
post(tree1_test, file = "C:/College/DATS6101/project/intro_ds_5/tree_test.ps",
     title = "Classification Tree for Diabetes - Testing Set")
```


```{r, comment = ""}
tree_pred_test <- predict(tree1_test, newdata=data_test, type = "class")
tree_pred_train <- predict(tree1, newdata=data_train, type="class")
cm3_test <- confusionMatrix(tree_pred_test, data_test$diabetes)

cm3_train <- confusionMatrix(tree_pred_train, data_train$diabetes)
print(cm3_train)
print(cm3_test)
acc3 <- mean(tree_pred_test == data_test$diabetes)
# come back to this later
```

```{r, comment = ""}
p3 <- round(100*(18759/(18759+5881)), digits = 1)
p3
r3 <- round(100*(18759/(18759+9482)), digits = 1)
r3
f13 <- 2*((p3*r3)/(p3+r3))
f13
```

```{r, comment = ""}
p4 <- round(100*(4706/(4706+1485)), digits = 1)
p4
r4 <- round(100*(4706/(4706+2399)), digits = 1)
r4
f14 <- 2*((p4*r4)/(p4+r4))
f14
```

Random Forests
```{r, comment = ""}
library(randomForest)

tree4 <- randomForest(diabetes ~ HighBP + HighChol + CholCheck + BMI + 
    Stroke + heart_dis + PhysActivity + Veggies + hvy_alc + GenHlth + 
    MentHlth + PhysHlth + DiffWalk + Sex + Age + Education + 
    Income, data=data_train)

print(tree4)
importance(tree4)
```

```{r, comment = ""}
tree4_pred_train <- predict(tree4, newdata=data_train)
cm4_train <- confusionMatrix(tree4_pred_train, data_train$diabetes)
acc4_train <- mean(tree4_pred_train == data_train$diabetes)
cm4_train
acc4_train
```

```{r, comment = ""}
source("C:/College/DATS6101/project/cf_functions.R")
tree4_pred_test <- predict(tree4, newdata=data_test)
cm4_test <- confusionMatrix(tree4_pred_test, data_test$diabetes)
cm4_draw <- draw_confusion_matrix(cm4_test)

```



Evaluation for Random Forests (can cut this out now that I can show all of this on the confusion matrix)


```{r, comment = ""}
p5 <- round(100*(27164/(27164+1738)), digits = 1)
p5
r5 <- round(100*(27164/(27164+1077)), digits = 1)
r5
f15 <- 2*((p5*r5)/(p5+r5))
f15
```

```{r, comment = ""}
p6 <- round(100*(4968/(4968+1503)), digits = 1)
p6
r6 <- round(100*(4968/(4968+2137)), digits = 1)
r6
f16 <- 2*((p6*r6)/(p6+r6))
f16
```
Since random forests' performance is too high on the training set compared to the testing set, I want to do this with 5 fold cross validation.

```{r, comment = ""}
set.seed(123)

# define repeated cross validation with 5 folds and 3 repeats

repeat_cv <- trainControl(method = 'repeatedcv', number=5, repeats=3)

set.seed(123)
forest<- train(diabetes ~ HighBP + HighChol + CholCheck + BMI + 
    Stroke + heart_dis + PhysActivity + Veggies + hvy_alc + GenHlth + 
    MentHlth + PhysHlth + DiffWalk + Sex + Age + Education + 
    Income, data=data_train, method='rf',trControl=repeat_cv, metric='Accuracy')
forest$finalModel
```

```{r}
forest_train <- predict(object=forest, newdata=data_train[, -1])
forest_test <- predict(object=forest, newdata=data_test[, -1])
acc_rf <- mean(forest_test == data_test$diabetes)*100
acc_rf
```

```{r, comment = ""}
cm5_train <- confusionMatrix(forest_train, data_train$diabetes)
cm5_test <- confusionMatrix(forest_test, data_test$diabetes)
cm5_draw1 <- draw_confusion_matrix(cm5_train)
cm5_draw2 <- draw_confusion_matrix(cm5_test)
```

KNN
