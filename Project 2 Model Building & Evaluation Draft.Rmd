---
title: "Project 2 Code"
author: "Erika Pham"
date: "2023-04-25"
output: 
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---



So first, I need to select which features go into the model for optimal results. From what we did last time, which is running chi-square tests on each possible predictor versus our dependent variable 'diabetes', we got that all of the included features have a relationship with the respondents' pre/diabetic status. 

From the notes, I want to try using the package written in the in-class example to see if I can get anything. I would have to recode the dependent variable as 'y'.

```{r, include = FALSE}
# loading in dataset
diabetes50 <- read.csv("diabetes_binary_5050split_health_indicators_BRFSS2015.csv")
str(diabetes50)

# renaming some longer variables
diabetes50 <- diabetes50 %>%
    rename("diabetes"= Diabetes_binary)
diabetes50 <- diabetes50 %>%
  rename("heart_dis" = HeartDiseaseorAttack)
diabetes50 <- diabetes50 %>%
  rename("hvy_alc" = HvyAlcoholConsump)

# storing categorical variables as factor
diabetes50$diabetes <- as.factor(diabetes50$diabetes)
diabetes50$HighBP <- as.factor(diabetes50$HighBP)
diabetes50$HighChol <- as.factor(diabetes50$HighChol)
diabetes50$CholCheck <- as.factor(diabetes50$CholCheck)
diabetes50$Smoker <- as.factor(diabetes50$Smoker)
diabetes50$Stroke <- as.factor(diabetes50$Stroke)
diabetes50$NoDocbcCost <- as.factor(diabetes50$NoDocbcCost)
diabetes50$heart_dis <- as.factor(diabetes50$heart_dis)
diabetes50$PhysActivity <- as.factor(diabetes50$PhysActivity)
diabetes50$Fruits <- as.factor(diabetes50$Fruits)
diabetes50$Veggies <- as.factor(diabetes50$Veggies)
diabetes50$hvy_alc <- as.factor(diabetes50$hvy_alc)
diabetes50$DiffWalk <- as.factor(diabetes50$DiffWalk)
diabetes50$Sex <- as.factor(diabetes50$Sex)
diabetes50$GenHlth <- as.factor(diabetes50$GenHlth)
diabetes50$AnyHealthcare <- as.factor(diabetes50$AnyHealthcare)
diabetes50$Education <- as.factor(diabetes50$Education)
diabetes50$Income <- as.factor(diabetes50$Income)
diabetes50$Age <- as.factor(diabetes50$Age)

```

Below, I'm separating the model into training and testing data. The ratio is 8:2.

```{r, comment = ""}
library(bestglm)
library(caret)
library(pROC)
library(glmnet)

set.seed(5000)
sample1 <- sample(c(TRUE, FALSE), nrow(diabetes50), replace = TRUE, prob=c(0.8,0.2))
data_train <- diabetes50[sample1, ]
data_test <- diabetes50[!sample1, ]


```


I'm going to write the full logistic regression model(s).

```{r, comment = ""}
full <- glm(diabetes~., data=diabetes50, family=binomial(link="logit"))
full_train <- glm(diabetes~., data=data_train, family=binomial(link="logit"))
full_test <- glm(diabetes~., data=data_test, family=binomial(link="logit"))
```

Let's perform stepwise selection to see the best model. 

```{r, comment=""}

# stepwise model, both directions
step1 <- stepAIC(full_train, direction = "both", trace = FALSE)
summary(step1)

```

Stepwise selection chose 16 variables. Let's fit the model again on= training data and then obtaining the predictive variable for diabetes on both data sets. Then we can evaluate the model. 




```{r, comment = ""}
library(DescTools)
train_model <- glm(formula = diabetes ~ HighBP + HighChol + CholCheck + BMI + 
    Stroke + heart_dis + PhysActivity + Veggies + hvy_alc + GenHlth + 
    MentHlth + PhysHlth + DiffWalk + Sex + Age + Education + 
    Income, family = binomial(link = "logit"), data = data_train)
summary(train_model)
exp(coef(train_model))
PseudoR2(train_model, which = "McFadden")
```



```{r}
test_model <- glm(formula = diabetes ~ HighBP + HighChol + CholCheck + BMI + 
    Stroke + heart_dis + PhysActivity + Veggies + hvy_alc + GenHlth + 
    MentHlth + PhysHlth + DiffWalk + Sex + Age + Education + 
    Income, family = binomial(link = "logit"), data = data_test)
summary(test_model)
PseudoR2(test_model, which = "McFadden")
```

Let's obtain the predicted value that the participant have diabetes on both training and testing set.
```{r, comment = ""}
library(pROC)
train_pred <- predict(train_model, newdata = data_train, type = "response")
data_train$train_pred = train_pred # this is adding a variable for the predicted value for diabetes
# maybe I can use this to use the ConfusionMatrixInfo etc in the example
test_pred <- predict(train_model, newdata = data_test, type = "response")
data_test$test_pred = test_pred

ggplot( data_train, aes( train_pred, color = as.factor(diabetes) ) ) + 
geom_density( size = 1 ) +
ggtitle( "Training Set's Predicted Score" ) + 
scale_color_economist( name = "data", labels = c( "negative", "positive" ) ) + 
theme_economist()
```
```{r}
ggplot( data_test, aes( test_pred, color = as.factor(diabetes) ) ) + 
geom_density( size = 1 ) +
ggtitle( "Testing Set's Predicted Score" ) + 
scale_color_economist( name = "data", labels = c( "negative", "positive" ) ) + 
theme_economist()
```


I want to look at an accuracy/confusion matrix. 

```{r, comment = ""}

cm1 <- table(data_train$diabetes, data_train$train_pred<.4)
xkabledply( cm1, title = "Confusion Matrix for Logistic Regression Model (reduced)" )
acc1_05 <- round(100*(cm1[1,1]+cm1[2,2])/sum(cm1), digits=1)
acc1_05
```

Accuracy is 25.2. Not good. What if I choose a different cutoff? 



Let's look at confusion  matrix for the model on the test data.
```{r, comment = ""}

cm2 <- table(data_test$diabetes, test_model$fitted.values<.4)
xkabledply( cm2, title = "Confusion Matrix for Logistic Regression Model (reduced)" )
acc2_05 <- round(100*(cm2[1,1]+cm2[2,2])/sum(cm2), digits=1)
acc2_05
```


Accuracy on test model is 25, which means it is performing similarly poorly (lol). At least it's not overfitting.
```{r}
# visualize .5 cutoff for training data
cm_info <- ConfusionMatrixInfo( data = data_train, predict = "train_pred", 
                                actual = "diabetes", cutoff = .5 )
ggthemr("flat")
cm_info$plot
```
```{r}
# visualize .44 cutoff for training data
cm_info <- ConfusionMatrixInfo( data = data_train, predict = "train_pred", 
                                actual = "diabetes", cutoff = .4 )
ggthemr("flat")
cm_info$plot
```


```{r}
# visualize .5 cutoff 
cm_info <- ConfusionMatrixInfo( data = data_test, predict = "test_pred", 
                                actual = "diabetes", cutoff = .5 )
ggthemr("flat")
cm_info$plot
```

```{r}
# visualize .4 cutoff 
cm_info <- ConfusionMatrixInfo( data = data_test, predict = "test_pred", 
                                actual = "diabetes", cutoff = .44 )
ggthemr("flat")
cm_info$plot
```
Let's see if finding the ROC can help with some evaluation and tuning. 

```{r, comment = ""}

roc_train1 <- roc(diabetes~train_pred, data=data_train)
plot(roc_train1)
auc(roc_train1)
```

```{r, comment = ""}

roc_test1 <- roc(diabetes~test_pred, data=data_test)
plot(roc_test1)
auc(roc_test1)
```

AUC is 0.825. This shouldn't happen because the data set is balanced. I have no idea why this is happening. 


I want to look at other metrics to see where to go next: precision, recall, f1 score

Precision : True Positive / (True Positive + False Positive)

Recall = TruePositives / (TruePositives + FalseNegatives)

F1 Score = 2 * (Precision * Recall) / (Precision + Recall)

```{r, comment = ""}
p1 <- round(100*(cm1[1,1]/(cm1[1,1]+cm1[1,2])), digits = 1)
p1
r1 <- round(100*(cm1[1,1]/(cm1[1,1]+cm1[2,1])), digits = 1)
r1
f11 <- 2*((p1*r1)/(p1+r1))
f11
```

```{r, comment = ""}
p2 <- round(100*(cm2[1,1]/(cm2[1,1]+cm2[1,2])), digits = 1)
p2
r2 <- round(100*(cm2[1,1]/(cm2[1,1]+cm2[2,1])), digits = 1)
r2
f12 <- 2*((p2*r2)/(p2+r2))
f12
```

```{r}
ROCR_pred_test <- prediction(train_pred, data_train$diabetes)
ROCR_perf_test <- performance(ROCR_pred_test,'tpr','fpr')
plot(ROCR_perf_test,colorize=TRUE,print.cutoffs.at=seq(0.1,by=0.1))
```

```{r}
cost_perf = performance(ROCR_pred_test, "cost") 
ROCR_pred_test@cutoffs[[1]][which.min(cost_perf@y.values[[1]])]
```

Tree time:

```{r}
library(rpart)
library(rpart.plot)
# grow tree
tree1 <- rpart(diabetes ~ HighBP + HighChol + CholCheck + BMI + 
    Stroke + heart_dis + PhysActivity + Veggies + hvy_alc + GenHlth + 
    MentHlth + PhysHlth + DiffWalk + Sex + Age + Education + 
    Income, method="class", data=data_train)
# display results
printcp(tree1)
# visualize cv results
plotcp(tree1)
# summary of splits
summary(tree1)

# plot tree
tree1_plot <- rpart.plot(tree1, uniform=TRUE, main="Classification Tree for Diabetes")
#text(tree1_plot, use.n=TRUE, all=TRUE, cex=.8) # this doesn't always work for some reason

# create pretty plot of tree
post(tree1, file = "C:/College/DATS6101/project/intro_ds_5/tree.ps",
     title = "Classification Tree for Diabetes")
```

```{r}
# prune the tree
tree2 <- prune(tree1, cp=tree1$cptable[which.min(tree1$cptable[,"xerror"]),"CP"])

# plot the tree
rpart.plot(tree2, uniform=T, main="Pruned Classification Tree for Diabetes")
#text(tree2, use.n=T, all=T, cex=.8)
post(tree2, file="C:/College/DATS6101/project/intro_ds_5/pruned_tree.ps", 
     title="Pruned Classification Tree for Diabetes")


```

```{r, comment = ""}
tree_pred_test <- predict(tree1, newdata=data_test, type = "class")
tree_pred_train <- predict(tree1, newdata=data_train, type="class")
cm3_test <- confusionMatrix(tree_pred_test, data_test$diabetes)

cm3_train <- confusionMatrix(tree_pred_train, data_train$diabetes)
print(cm3_train)
print(cm3_test)
acc3 <- mean(tree_pred == data_test$diabetes)
# come back to this later
```

Random Forests
```{r, comment = ""}
library(randomForest)

tree4 <- randomForest(diabetes ~ HighBP + HighChol + CholCheck + BMI + 
    Stroke + heart_dis + PhysActivity + Veggies + hvy_alc + GenHlth + 
    MentHlth + PhysHlth + DiffWalk + Sex + Age + Education + 
    Income, data=data_train)

print(tree4)
importance(tree4)
```

```{r, comment = ""}
tree4_pred <- predict(tree4, newdata=data_test)
cm4 <- confusionMatrix(tree4_pred, data_test$diabetes)
acc4 <- mean(tree4_pred == data_test$diabetes)
cm4
acc4 
```

Add some CV validation here for trees
