---
title: "Project 2 Code"
author: "Erika Pham"
date: "2023-04-25"
output: 
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F, message = F)
options(scientific = T, digits = 3)
```

```{r init, include = FALSE}
# loading in libraries
library(ezids)
library(tidyverse)
library(readr)
library(ggplot2)
library(stringr)
library(gridExtra)
library(MASS)
library(rpart)
library(caret)
library(pROC)
library(leaps)
library(ISLR)
library(ggthemes)
library(data.table)
library(ggthemr)
library(ROCR)
library(ggpubr)
library(grid)
```

```{r, include = FALSE}
# colorblind color palettes

# The palette with grey:
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# The palette with black:
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# To use for fills, add
  #scale_fill_manual(values=cbPalette)

# To use for line and point colors, add
  #scale_colour_manual(values=cbPalette)
```

```{r}
# ------------------------------------------------------------------------------------------
# [AccuracyCutoffInfo] : 
# Obtain the accuracy on the trainining and testing dataset.
# for cutoff value ranging from .4 to .8 ( with a .05 increase )
# @train   : your data.table or data.frame type training data ( assumes you have the predicted score in it ).
# @test    : your data.table or data.frame type testing data
# @predict : prediction's column name (assumes the same for training and testing set)
# @actual  : actual results' column name
# returns  : 1. data : a data.table with three columns.
#            		   each row indicates the cutoff value and the accuracy for the 
#            		   train and test set respectively.
# 			 2. plot : plot that visualizes the data.table


AccuracyCutoffInfo <- function( train, test, predict, actual )
{
	# change the cutoff value's range as you please 
	cutoff <- seq( .4, .8, by = .05 )

	accuracy <- lapply( cutoff, function(c)
	{
		# use the confusionMatrix from the caret package
	  data_train <- as.factor( as.numeric( train[[predict]] > c ) )
		cm_train <- confusionMatrix(data_train, as.factor(train[[actual]]) )
		data_test <- as.factor( as.numeric( test[[predict]] > c ) )
		cm_test  <- confusionMatrix( data_test, as.factor(test[[actual]]) )
			
		dt <- data.table( cutoff = c,
						  train  = cm_train$overall[["Accuracy"]],
		 			      test   = cm_test$overall[["Accuracy"]] )
		return(dt)
	}) %>% rbindlist()

	# visualize the accuracy of the train and test set for different cutoff value 
	# accuracy in percentage.
	accuracy_long <- gather( accuracy, "data", "accuracy", -1 )
	
	plot <- ggplot( accuracy_long, aes( cutoff, accuracy, group = data, color = data ) ) + 
			geom_line( size = 1 ) + geom_point( size = 3 ) +
			scale_y_continuous( label = percent ) +
			ggtitle( "Train/Test Accuracy for Different Cutoff" )

	return( list( data = accuracy, plot = plot ) )
}


# ------------------------------------------------------------------------------------------
# [ConfusionMatrixInfo] : 
# Obtain the confusion matrix plot and data.table for a given
# dataset that already consists the predicted score and actual outcome.
# @data    : your data.table or data.frame type data that consists the column
#            of the predicted score and actual outcome 
# @predict : predicted score's column name
# @actual  : actual results' column name
# @cutoff  : cutoff value for the prediction score 
# return   : 1. data : a data.table consisting of three column
#            		   the first two stores the original value of the prediction and actual outcome from
#			 		   the passed in data frame, the third indicates the type, which is after choosing the 
#			 		   cutoff value, will this row be a true/false positive/ negative 
#            2. plot : plot that visualizes the data.table 

ConfusionMatrixInfo <- function( data, predict, actual, cutoff )
{	
	# extract the column ;
	# relevel making 1 appears on the more commonly seen position in 
	# a two by two confusion matrix	
	predict <- data[[predict]]
	actual  <- relevel( as.factor( data[[actual]] ), "1" )
	
	result <- data.table( actual = actual, predict = predict )

	# calculating each pred falls into which category for the confusion matrix
	result[ , type := ifelse( predict >= cutoff & actual == 1, "TP",
					  ifelse( predict >= cutoff & actual == 0, "FP", 
					  ifelse( predict <  cutoff & actual == 1, "FN", "TN" ) ) ) %>% as.factor() ]

	# jittering : can spread the points along the x axis 
	plot <- ggplot( result, aes( actual, predict, color = type ) ) + 
			geom_violin( fill = "white", color = NA ) +
			geom_jitter( shape = 1 ) + 
			geom_hline( yintercept = cutoff, color = "blue", alpha = 0.6 ) + 
			scale_y_continuous( limits = c( 0, 1 ) ) + 
			scale_color_discrete( breaks = c( "TP", "FN", "FP", "TN" ) ) + # ordering of the legend 
			guides( col = guide_legend( nrow = 2 ) ) + # adjust the legend to have two rows  
			ggtitle( sprintf( "Confusion Matrix with Cutoff at %.2f", cutoff ) )

	return( list( data = result, plot = plot ) )
}


# ------------------------------------------------------------------------------------------
# [ROCInfo] : 
# Pass in the data that already consists the predicted score and actual outcome.
# to obtain the ROC curve 
# @data    : your data.table or data.frame type data that consists the column
#            of the predicted score and actual outcome
# @predict : predicted score's column name
# @actual  : actual results' column name
# @cost.fp : associated cost for a false positive 
# @cost.fn : associated cost for a false negative 
# return   : a list containing  
#			 1. plot        : a side by side roc and cost plot, title showing optimal cutoff value
# 				 	   		  title showing optimal cutoff, total cost, and area under the curve (auc)
# 		     2. cutoff      : optimal cutoff value according to the specified fp/fn cost 
#		     3. totalcost   : total cost according to the specified fp/fn cost
#			 4. auc 		: area under the curve
#		     5. sensitivity : TP / (TP + FN)
#		     6. specificity : TN / (FP + TN)

ROCInfo <- function( data, predict, actual, cost.fp, cost.fn )
{
	# calculate the values using the ROCR library
	# true positive, false postive 
	pred <- prediction( data[[predict]], data[[actual]] )
	perf <- performance( pred, "tpr", "fpr" )
	roc_dt <- data.frame( fpr = perf@x.values[[1]], tpr = perf@y.values[[1]] )

	# cost with the specified false positive and false negative cost 
	# false postive rate * number of negative instances * false positive cost + 
	# false negative rate * number of positive instances * false negative cost
	cost <- perf@x.values[[1]] * cost.fp * sum( data[[actual]] == 0 ) + 
			( 1 - perf@y.values[[1]] ) * cost.fn * sum( data[[actual]] == 1 )

	cost_dt <- data.frame( cutoff = pred@cutoffs[[1]], cost = cost )

	# optimal cutoff value, and the corresponding true positive and false positive rate
	best_index  <- which.min(cost)
	best_cost   <- cost_dt[ best_index, "cost" ]
	best_tpr    <- roc_dt[ best_index, "tpr" ]
	best_fpr    <- roc_dt[ best_index, "fpr" ]
	best_cutoff <- pred@cutoffs[[1]][ best_index ]
	
	# area under the curve
	auc <- performance( pred, "auc" )@y.values[[1]]

	# normalize the cost to assign colors to 1
	normalize <- function(v) ( v - min(v) ) / diff( range(v) )
	
	# create color from a palette to assign to the 100 generated threshold between 0 ~ 1
	# then normalize each cost and assign colors to it, the higher the blacker
	# don't times it by 100, there will be 0 in the vector
	col_ramp <- colorRampPalette( c( "green", "orange", "red", "black" ) )(100)   
	col_by_cost <- col_ramp[ ceiling( normalize(cost) * 99 ) + 1 ]

	roc_plot <- ggplot( roc_dt, aes( fpr, tpr ) ) + 
				geom_line( color = rgb( 0, 0, 1, alpha = 0.3 ) ) +
				geom_point( color = col_by_cost, size = 4, alpha = 0.2 ) + 
				geom_segment( aes( x = 0, y = 0, xend = 1, yend = 1 ), alpha = 0.8, color = "royalblue" ) + 
				labs( title = "ROC", x = "False Postive Rate", y = "True Positive Rate" ) +
				geom_hline( yintercept = best_tpr, alpha = 0.8, linetype = "dashed", color = "steelblue4" ) +
				geom_vline( xintercept = best_fpr, alpha = 0.8, linetype = "dashed", color = "steelblue4" )				

	cost_plot <- ggplot( cost_dt, aes( cutoff, cost ) ) +
				 geom_line( color = "blue", alpha = 0.5 ) +
				 geom_point( color = col_by_cost, size = 4, alpha = 0.5 ) +
				 ggtitle( "Cost" ) +
				 scale_y_continuous(labels=waiver()) +
				 geom_vline( xintercept = best_cutoff, alpha = 0.8, linetype = "dashed", color = "steelblue4" )	

	# the main title for the two arranged plot
	sub_title <- sprintf( "Cutoff at %.2f - Total Cost = %d, AUC = %.3f", 
						  best_cutoff, best_cost, auc )
	
	# arranged into a side by side plot
	plot <- arrangeGrob( roc_plot, cost_plot, ncol = 2, 
						 top = textGrob( sub_title, gp = gpar( fontsize = 16, fontface = "bold" ) ) )
	
	return( list( plot 		  = plot, 
				  cutoff 	  = best_cutoff, 
				  totalcost   = best_cost, 
				  auc         = auc,
				  sensitivity = best_tpr, 
				  specificity = 1 - best_fpr ) )
}
```

So first, I need to select which features go into the model for optimal results. From what we did last time, which is running chi-square tests on each possible predictor versus our dependent variable 'diabetes', we got that all of the included features have a relationship with the respondents' pre/diabetic status. 

From the notes, I want to try using the package written in the in-class example to see if I can get anything. I would have to recode the dependent variable as 'y'.

```{r, include = FALSE}
# loading in dataset
diabetes50 <- read.csv("diabetes_binary_5050split_health_indicators_BRFSS2015.csv")
str(diabetes50)

# renaming some longer variables
diabetes50 <- diabetes50 %>%
    rename("diabetes"= Diabetes_binary)
diabetes50 <- diabetes50 %>%
  rename("heart_dis" = HeartDiseaseorAttack)
diabetes50 <- diabetes50 %>%
  rename("hvy_alc" = HvyAlcoholConsump)

# storing categorical variables as factor
diabetes50$diabetes <- as.factor(diabetes50$diabetes)
diabetes50$HighBP <- as.factor(diabetes50$HighBP)
diabetes50$HighChol <- as.factor(diabetes50$HighChol)
diabetes50$CholCheck <- as.factor(diabetes50$CholCheck)
diabetes50$Smoker <- as.factor(diabetes50$Smoker)
diabetes50$Stroke <- as.factor(diabetes50$Stroke)
diabetes50$NoDocbcCost <- as.factor(diabetes50$NoDocbcCost)
diabetes50$heart_dis <- as.factor(diabetes50$heart_dis)
diabetes50$PhysActivity <- as.factor(diabetes50$PhysActivity)
diabetes50$Fruits <- as.factor(diabetes50$Fruits)
diabetes50$Veggies <- as.factor(diabetes50$Veggies)
diabetes50$hvy_alc <- as.factor(diabetes50$hvy_alc)
diabetes50$DiffWalk <- as.factor(diabetes50$DiffWalk)
diabetes50$Sex <- as.factor(diabetes50$Sex)
diabetes50$GenHlth <- as.factor(diabetes50$GenHlth)
diabetes50$AnyHealthcare <- as.factor(diabetes50$AnyHealthcare)
diabetes50$Education <- as.factor(diabetes50$Education)
diabetes50$Income <- as.factor(diabetes50$Income)

```


Below, I'm separating the model into training and testing data. The ratio is 70:30.

```{r, comment = ""}
library(bestglm)
library(caret)
library(pROC)
library(glmnet)

set.seed(100)
sample <- sample(c(TRUE, FALSE), nrow(diabetes50), replace = TRUE, prob=c(0.7,0.3))
data_train <- diabetes50[sample, ]
data_test <- diabetes50[!sample, ]


```

I'm going to write the full logistic regression model(s).

```{r, comment = ""}
full <- glm(diabetes~., data=diabetes50, family=binomial(link="logit"))
full_train <- glm(diabetes~., data=data_train, family=binomial(link="logit"))
full_test <- glm(diabetes~., data=data_test, family=binomial(link="logit"))
```

Let's perform stepwise selection to see the best model. 

```{r, comment=""}

# stepwise model, both directions
step1 <- stepAIC(full_train, direction = "both", trace = FALSE)
summary(step1)

```

Stepwise selection chose 17 variables. Let's fit the model again on= training data and then obtaining the predictive variable for diabetes on both data sets. Then we can evaluate the model. 

```{r, comment = ""}
library(DescTools)
train_model <- glm(formula = diabetes ~ HighBP + HighChol + CholCheck + BMI + 
    Stroke + heart_dis + PhysActivity + Veggies + hvy_alc + GenHlth + 
    MentHlth + PhysHlth + DiffWalk + Sex + Age + Education + 
    Income, family = binomial(link = "logit"), data = data_train)
summary(train_model)
PseudoR2(train_model, which = "McFadden")
```

```{r}
test_model <- glm(formula = diabetes ~ HighBP + HighChol + CholCheck + BMI + 
    Stroke + heart_dis + PhysActivity + Veggies + hvy_alc + GenHlth + 
    MentHlth + PhysHlth + DiffWalk + Sex + Age + Education + 
    Income, family = binomial(link = "logit"), data = data_test)
summary(test_model)
PseudoR2(test_model, which = "McFadden")
```

Let's obtain the predicted value that the participant have diabetes on both training and testing set.
```{r, comment = ""}
library(pROC)
train_pred <- predict(train_model, newdata = data_train, type = "response")
data_train$train_pred = train_pred # this is adding a variable for the predicted value for diabetes
# maybe I can use this to use the ConfusionMatrixInfo etc in the example
test_pred <- predict(train_model, newdata = data_test, type = "response")
data_test$test_pred = test_pred

ggplot( data_train, aes( train_pred, color = as.factor(diabetes) ) ) + 
geom_density( size = 1 ) +
ggtitle( "Training Set's Predicted Score" ) + 
scale_color_economist( name = "data", labels = c( "negative", "positive" ) ) + 
theme_economist()
```

I want to look at an accuracy/confusion matrix. 

```{r, comment = ""}

cm1 <- table(data_train$diabetes, data_train$train_pred<.5)
xkabledply( cm1, title = "Confusion Matrix for Logistic Regression Model (reduced)" )
acc1_05 <- round(100*(cm1[1,1]+cm1[2,2])/sum(cm1), digits=1)
acc1_05
```

Accuracy is 25.2. Not good. What if I choose a different cutoff? 


Let's look at confusion  matrix for the model on the test data.
```{r, comment = ""}

cm2 <- table(data_test$diabetes, test_model$fitted.values<.5)
xkabledply( cm2, title = "Confusion Matrix for Logistic Regression Model (reduced)" )
acc2_05 <- round(100*(cm2[1,1]+cm2[2,2])/sum(cm2), digits=1)
acc2_05
```

Accuracy on test model is 25, which means it is performing similarly poorly (lol). At least it's not overfitting.

```{r}
# visualize .6 cutoff (lowest point of the previous plot)
cm_info <- ConfusionMatrixInfo( data = data_test, predict = "test_pred", 
                                actual = "diabetes", cutoff = .4 )
ggthemr("flat")
cm_info$plot
```
Let's see if finding the ROC can help with some evaluation and tuning. 

```{r, comment = ""}

roc_train1 <- roc(diabetes~train_pred, data=train)
plot(roc_train1)
auc(roc_train1)
```

AUC is 0.825. This shouldn't happen because the data set is balanced. I have no idea why this is happening. 


I want to look at other metrics to see where to go next: precision, recall, f1 score

Precision : True Positive / (True Positive + False Positive)

Recall = TruePositives / (TruePositives + FalseNegatives)

F1 Score = 2 * (Precision * Recall) / (Precision + Recall)

```{r, comment = ""}
p1 <- round(100*(cm1[1,1]/(cm1[1,1]+cm1[1,2])), digits = 1)
p1
r1 <- round(100*(cm1[1,1]/(cm1[1,1]+cm1[2,1])), digits = 1)
r1
f11 <- 2*(p1*r1)/(p1+r1)
f11
```

```{r}
ROCR_pred_test <- prediction(test_pred, data_test$diabetes)
ROCR_perf_test <- performance(ROCR_pred_test,'tpr','fpr')
plot(ROCR_perf_test,colorize=TRUE,print.cutoffs.at=seq(0.1,by=0.1))
```
```{r}
cost_perf = performance(ROCR_pred_test, "cost") 
ROCR_pred_test@cutoffs[[1]][which.min(cost_perf@y.values[[1]])]
```